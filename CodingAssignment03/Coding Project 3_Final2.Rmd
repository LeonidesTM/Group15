---
title: "Coding Assignment 3"
author: "Team 15"
date: "Due: 2023-12-09 23:59"
output: 
pdf_document:
toc: true
---

```{r setup, include=FALSE}
#Put any packages you need here
library(tidyverse)
library(lmtest)
library(readxl)
library(plotly)
library(corrplot)
library(dplyr) # for pipe operator
library(gt) # for fancier tables
library(gtsummary) # for fancier summary statistics
library(corrplot) # for fancier correlations
library(car) # for easier scatterplots
library(jtools) # for fancier regression output
knitr::opts_chunk$set(echo = TRUE)
```

A Florida health insurance company wants to predict annual claims for individual clients. The company pulls a random sample of 100 customers. The owner wishes to charge an actuarially fair premium to ensure a normal rate of return. The owner collects all of their current customer’s health care expenses from the last year and compares them with what is known about each customer’s plan. 

The data on the 100 customers in the sample is as follows:
  
  -	Charges: Total medical expenses for a particular insurance plan (in dollars)
-	Age: Age of the primary beneficiary
-	BMI: Primary beneficiary’s body mass index (kg/m2)
-	Female: Primary beneficiary’s birth sex (0 = Male, 1 = Female)
-	Children: Number of children covered by health insurance plan (includes other dependents as well)
-	Smoker: Indicator if primary beneficiary is a smoker (0 = non-smoker, 1 = smoker)
-	Cities: Dummy variables for each city with the default being Sanford

Answer the following questions using complete sentences and attach all output, plots, etc. within this report.


```{r dataset, echo=TRUE}
# Bring in the dataset here.
read.csv("~/Documents/Insurance_Data_Group15.csv")
# Bring in the dataset here.
Insurance_Data_Group15 <- read.csv("~/Documents/Insurance_Data_Group15.csv")
```

## Question 1

Randomly select 30 observations from the sample and exclude from all modeling (i.e. n=47). Provide the summary statistics (min, max, std, mean, median) of the quantitative variables for the 70 observations.

```{r q1}
#
set.seed(123457)
index <- sample(seq_len(nrow(Insurance_Data_Group15)), size = 30)
Sample <- Insurance_Data_Group15[-index,]
Else <- Insurance_Data_Group15[index,]
Quantatative <- Sample[, c("Age", "BMI", "Children")]
Quantatative %>%
tbl_summary(statistic = list(all_continuous() ~ c("{mean} ({sd})",
                                                  "{median} ({p25}, {p75})",
                                                  "{min}, {max}"),
                             all_categorical() ~ "{n} / {N}"),
            type = all_continuous() ~ "continuous2"
            )
```


## Question 2

Provide the correlation between all quantitative variables
```{r q2}
Quantatative <- Sample[, c("Age", "BMI", "Children")]
correlation_matrix <- cor(Quantatative)
print(correlation_matrix)
```
## Question 3
Run a regression that includes all independent variables in the data table. Does the model above violate any of the Gauss-Markov assumptions? If so, what are they and what is the solution for correcting?
```{r q3}
#Linear Regression for all independent variables
Model_All_Variables <- lm(Charges ~ Age + BMI + Children + Female + Smoker + WinterSprings + WinterPark + Oviedo, data = Sample)
summary(Model_All_Variables)
par(mfrow=c(2,2))
plot(Model_All_Variables)
``` 
## Results
Based on the plot titled "Residuals vs Fitted", it would appear as if there is a nonlinear pattern, indicating a violation of the 3rd assumption.

In the second plot, "Normal Q-Q", you see the assumption of uniformly distributed set of residuals. Since it deviates from the line, we know the 4th assumption may be violated

## Question 4
  
  Implement the solutions from question 3, such as data transformation, along with any other changes you wish. Use the sample data and run a new regression. How have the fit measures changed? How have the signs and significance of the coefficients changed?
  
```{r q4}
#BMI
par(mfrow=c(1,2))
hist(Quantatative$BMI) #Before
Log_BMI <- log(Quantatative$BMI)
hist(Log_BMI) #After
#Age
par(mfrow=c(1,2))
hist(Quantatative$Age) #Before
Log_Age <- log(Quantatative$Age)
hist(Log_Age) #After
#Transforming Just BMI
Model_All_Variables_BMI_Transformed <- lm(Charges ~ Age + Log_BMI + Children + Female + Smoker + WinterSprings + WinterPark + Oviedo, data = Sample)
summary(Model_All_Variables_BMI_Transformed)
#Transforming Age and BMI
Model_All_Variables_Both_Transformed <- lm(Charges ~ Log_Age + Log_BMI + Children + Female + Smoker + WinterSprings + WinterPark + Oviedo, data = Sample)
summary(Model_All_Variables_Both_Transformed)
```
## Answer
We are running a logarithm on BMI and Age since those are the things that would intuitively put more stress on the healthcare system and result in more frequent visits to a PCP, resulting in nonlinear charges as those things go up. Running a log on BMI results in a more normally distributed set of values per the histogram. However, running a natural log on Age did not seem to do much for the distribution.It seems to have gone from fairly normal already to more normal. 

For the regression, we ran two models: one in which we only transform BMI, since it had the bigger change in distribution per the histograms and then a model of both BMI and Age transformed. 

Original: R^2 = 0.7598

Just BMI: better fit than original. No coefficient sign changes of variables that tested significant on 2-tailed test . R^2 = 0.7604

BMI and Age: worse fit than original by a little bit. Also no coefficient sign changes of variables that tested significant on 2 tailed test. R^2 = 0.7401


## Question 5

Use the 30 withheld observations and calculate the performance measures for your best two models. Which is the better model? (remember that "better" depends on whether your outlook is short or long run)

```{r q5}
#Defining Values for new N = 100 Data Set
Log_Age_Total <- log(Insurance_Data_Group15$Age)
Log_BMI_Total <- log(Insurance_Data_Group15$BMI)
Model_All_Variables_BMI_Transformed_Total <- lm(Charges ~ Age + Log_BMI_Total + Children + Female + Smoker + WinterSprings + WinterPark + Oviedo, data = Insurance_Data_Group15)
Model_All_Variables_Both_Transformed_Total <- lm(Charges ~ Log_Age_Total + Log_BMI_Total + Children + Female + Smoker + WinterSprings + WinterPark + Oviedo, data = Insurance_Data_Group15)
# Finding Error
Original_Model <- predict(Model_All_Variables, newdata = Insurance_Data_Group15)
Model1_Pred <- predict(Model_All_Variables_BMI_Transformed_Total, newdata = Insurance_Data_Group15)
Model2_Pred <- predict(Model_All_Variables_Both_Transformed_Total, newdata = Insurance_Data_Group15)
Error_OG <- Original_Model - Insurance_Data_Group15$BMI
Error_1 <- Model1_Pred - Insurance_Data_Group15$BMI
Error_2 <- Model2_Pred - Insurance_Data_Group15$BMI
#Bias
mean(Error_OG)
mean(Error_1)
mean(Error_2)
#MAE
mae <- function(error_vector){error_vector %>% 
    abs() %>%
    mean()
}
mae(Error_OG)
mae(Error_1)
mae(Error_2)

#RMSE
rmse <- function (error_vector){
  error_vector^2 %>%
  mean() %>%
  sqrt()
}
rmse(Error_OG)
rmse(Error_1)
rmse(Error_2)

#MAPE
mape <- function(error_vector, actual_vector){
  (error_vector/actual_vector) %>%
  abs() %>%
  mean()
}
mape(Error_OG, Insurance_Data_Group15$BMI)
mape(Error_1, Insurance_Data_Group15$BMI)
mape(Error_2, Insurance_Data_Group15$BMI)
```
## Answer
Bias
Original: 12440.17
Model 1: 12350.79
Model 2: 12350.79
MAE
Original: 12515.01
Model 1: 12394.17
Model 2: 12403.52
RMSE
Original: 15767.58
Model 1: 15916.3
Model 2: 15858.32
MAPE
Original: 426.8173
Model 1: 422.0062
Model 2: 422.4184

Model 1 and 2 yielded identical Bias and nearly identical MAPE. However, despite having a higher level of bias, MAE, and MAPE, our original model yielded the best RMSE, indicating a better ability to minimize the larger errors. In the short-run, our original model might actually be the best predictor. However, in the long-run, it would probably be Model 2.

## Question 6

Provide interpretations of the coefficients, do the signs make sense? Perform marginal change analysis (thing 2) on the independent variables.

```{r q6}
#Model 1
# Extract coefficients
coefficients <- coef(Model_All_Variables_BMI_Transformed)
# Identify the coefficient of interest
coefficient_of_Age <- coefficients["Age"]
coefficient_of_Log_BMI <- coefficients["Log_BMI"]
coefficient_of_Children <- coefficients["Children"]
coefficient_of_Female <- coefficients["Female"]
coefficient_of_Smoker <- coefficients["Smoker"]
coefficient_of_WinterSprings <- coefficients["WinterSprings"]
coefficient_of_WinterPark <- coefficients["WinterPark"]
coefficient_of_Oviedo <- coefficients["Oviedo"]
# Interpret the coefficient
interpretation_Age <- paste("A one-unit change in Age is associated with a change of", round(coefficient_of_Age, 3), "units in the predicted response for our first model, holding other variables constant.")
print(interpretation_Age)
interpretation_Log_BMI <- paste("A one-unit change in Log BMI is associated with a change of", round(coefficient_of_Log_BMI, 3), "units in the predicted response for our first model, holding other variables constant.")
print(interpretation_Log_BMI)
interpretation_Children <- paste("A one-unit change in Children is associated with a change of", round(coefficient_of_Children, 3), "units in the predicted response for our first model, holding other variables constant.")
print(interpretation_Children)
interpretation_Female <- paste("A one-unit change in Female is associated with a change of", round(coefficient_of_Female, 3), "units in the predicted response for our first model, holding other variables constant.")
print(interpretation_Female)
interpretation_Smoker <- paste("A one-unit change in Smoker is associated with a change of", round(coefficient_of_Smoker, 3), "units in the predicted response for our first model, holding other variables constant.")
print(interpretation_Smoker)
interpretation_WinterSprings <- paste("A one-unit change in Winter Springs is associated with a change of", round(coefficient_of_WinterSprings, 3), "units in the predicted response for our first model, holding other variables constant.")
print(interpretation_WinterSprings)
interpretation_WinterPark <- paste("A one-unit change in Winter Park is associated with a change of", round(coefficient_of_WinterPark, 3), "units in the predicted response for our best model, holding other variables constant.")
print(interpretation_WinterPark)
interpretation_Oviedo <- paste("A one-unit change in Femlale is associated with a change of", round(coefficient_of_Oviedo, 3), "units in the predicted response for our first model, holding other variables constant.")
print(interpretation_Oviedo)
#Model 2
# Extract coefficients
coefficients2 <- coef(Model_All_Variables_Both_Transformed)
# Identify the coefficient of interest
coefficient2_of_Log_Age <- coefficients2["Log_Age"]
coefficient2_of_Log_BMI <- coefficients2["Log_BMI"]
coefficient2_of_Children <- coefficients2["Children"]
coefficient2_of_Female <- coefficients2["Female"]
coefficient2_of_Smoker <- coefficients2["Smoker"]
coefficient2_of_WinterSprings <- coefficients2["WinterSprings"]
coefficient2_of_WinterPark <- coefficients2["WinterPark"]
coefficient2_of_Oviedo <- coefficients2["Oviedo"]
# Interpret the coefficient
interpretation2_Log_Age <- paste("A one-unit change in Log Age is associated with a change of", round(coefficient2_of_Log_Age, 3), "units in the predicted response in our second model, holding other variables constant.")
print(interpretation2_Log_Age)
interpretation2_Log_BMI <- paste("A one-unit change in Log BMI is associated with a change of", round(coefficient2_of_Log_BMI, 3), "units in the predicted response in our second model, holding other variables constant.")
print(interpretation2_Log_BMI)
interpretation2_Children <- paste("A one-unit change in Children is associated with a change of", round(coefficient2_of_Children, 3), "units in the predicted response in our second model, holding other variables constant.")
print(interpretation2_Children)
interpretation2_Female <- paste("A one-unit change in Female is associated with a change of", round(coefficient2_of_Female, 3), "units in the predicted response in our second model, holding other variables constant.")
print(interpretation2_Female)
interpretation2_Smoker <- paste("A one-unit change in Smoker is associated with a change of", round(coefficient2_of_Smoker, 3), "units in the predicted response in our second model, holding other variables constant.")
print(interpretation2_Smoker)
interpretation2_WinterSprings <- paste("A one-unit change in Winter Springs is associated with a change of", round(coefficient2_of_WinterSprings, 3), "units in the predicted response in our second model, holding other variables constant.")
print(interpretation2_WinterSprings)
interpretation2_WinterPark <- paste("A one-unit change in Winter Park is associated with a change of", round(coefficient2_of_WinterPark, 3), "units in the predicted response in our second model, holding other variables constant.")
print(interpretation2_WinterPark)
interpretation2_Oviedo <- paste("A one-unit change in Oviedo is associated with a change of", round(coefficient2_of_Oviedo, 3), "units in the predicted response in our second model, holding other variables constant.")
print(interpretation2_Oviedo)
```
## Answer
The coefficients are all what we would have expected them to be with the exception of the cities (Winter Springs, Winter Park, and Oviedo). However, those independent variables did not test significant so we will not worry about them in our data interpretation. 

## Question 7

An eager insurance representative comes back with five potential clients. Using the better of the two models selected above, provide the prediction intervals for the five potential clients using the information provided by the insurance rep.

| Customer | Age | BMI | Female | Children | Smoker | City           |
  | -------- | --- | --- | ------ | -------- | ------ | -------------- | 
  | 1        | 60  | 22  | 1      | 0        | 0      | Oviedo         |
  | 2        | 40  | 30  | 0      | 1        | 0      | Sanford        |
  | 3        | 25  | 25  | 0      | 0        | 1      | Winter Park    |
  | 4        | 33  | 35  | 1      | 2        | 0      | Winter Springs |
  | 5        | 45  | 27  | 1      | 3        | 0      | Oviedo         |
  
  
```{r q7}
Prediction1 <- data.frame(Age = 60, Log_BMI = 22, Children = 0, Female = 1, Smoker = 0, Sanford = 0, WinterSprings = 0, WinterPark = 0, Oviedo = 1)
predict(Model_All_Variables_BMI_Transformed, newdata = Prediction1, interval = "confidence", level = .95)
Prediction2 <- data.frame(Age = 40, Log_BMI = 30, Children = 1, Female = 0, Smoker = 0, Sanford = 1, WinterSprings = 0, WinterPark = 0, Oviedo = 0)
predict(Model_All_Variables_BMI_Transformed, newdata = Prediction2, interval = "confidence", level = .95)
Prediction3 <- data.frame(Age = 25, Log_BMI = 25, Children = 0, Female = 0, Smoker = 1, Sanford = 0, WinterSprings = 0, WinterPark = 1, Oviedo = 0)
predict(Model_All_Variables_BMI_Transformed, newdata = Prediction3, interval = "confidence", level = .95)
Prediction4 <- data.frame(Age = 33, Log_BMI = 35, Children = 2, Female = 1, Smoker = 0, Sanford = 0, WinterSprings = 1, WinterPark = 0, Oviedo = 0)
predict(Model_All_Variables_BMI_Transformed, newdata = Prediction4, interval = "confidence", level = .95)
Prediction5 <- data.frame(Age = 45, Log_BMI = 27, Children = 3, Female = 1, Smoker = 0, Sanford = 0, WinterSprings = 0, WinterPark = 0, Oviedo = 1)
predict(Model_All_Variables_BMI_Transformed, newdata = Prediction5, interval = "confidence", level = .95)
```
## Answer
Prediction 1: (18641.82, 265410) at 95% confidence
Prediction 2: (22213.45, 373874.4) at 95% confidence
Prediction 3: (38954.46, 322864.9) at 95% confidence
Prediction 4: (27485.36, 445550.5) at 95% confidence
Prediction 5: (23530.06, 336470.2) at 95% confidence

## Question 8

The owner notices that some of the predictions are wider than others, explain why.

## Answer

It is possible that our sample size is too small. More data points might provide a better fit , tighter residuals, and a narrower prediction interval. Additionally, despite transforming the data, there could still be a bit of heteroskedasticity causing the errors to on average not uniform. Some of this might be caused by the presence of insignificant independent variables. 

## Question 9 

Are there any prediction problems that occur with the five potential clients? If so, explain.

## Answer
Our prediction intervals seem to be extremely wide. With a range at times ranging over $200,000, it is worth asking if such a wide prediction interval is even useful. As stated above, it may be worth considering if this is due to the presence of independent variables that did not test significant 